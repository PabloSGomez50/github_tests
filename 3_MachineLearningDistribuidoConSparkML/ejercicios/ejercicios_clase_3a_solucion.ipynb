{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Solucion"
      ],
      "metadata": {
        "id": "W9jQlnnDZUZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalamos PySpark"
      ],
      "metadata": {
        "id": "T_LdkaDVZVqh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfgpl3qAZHKV",
        "outputId": "e14e1861-18cf-4a58-da13-c657cac31a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bajamos los datos:"
      ],
      "metadata": {
        "id": "7UBQ1v1NZRWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/datasciencedojo/datasets/raw/master/titanic.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh6EarcLZK6-",
        "outputId": "1f276010-944a-4b2f-e81f-e6059cda1347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-04 23:43:29--  https://github.com/datasciencedojo/datasets/raw/master/titanic.csv\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv [following]\n",
            "--2023-08-04 23:43:29--  https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60302 (59K) [text/plain]\n",
            "Saving to: ‘titanic.csv’\n",
            "\n",
            "\rtitanic.csv           0%[                    ]       0  --.-KB/s               \rtitanic.csv         100%[===================>]  58.89K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-08-04 23:43:29 (39.1 MB/s) - ‘titanic.csv’ saved [60302/60302]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos la sesion de Spark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "GvpkhqFGZaXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('titanic.csv')[['Pclass', 'Survived']]\n",
        "raw_df = spark.createDataFrame(df)\n",
        "raw_df.show()"
      ],
      "metadata": {
        "id": "YVnv07G9ZfM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa1a14d-7231-448f-a1d0-470a4089b6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+\n",
            "|Pclass|Survived|\n",
            "+------+--------+\n",
            "|     3|       0|\n",
            "|     1|       1|\n",
            "|     3|       1|\n",
            "|     1|       1|\n",
            "|     3|       0|\n",
            "|     3|       0|\n",
            "|     1|       0|\n",
            "|     3|       0|\n",
            "|     3|       1|\n",
            "|     2|       1|\n",
            "|     3|       1|\n",
            "|     1|       1|\n",
            "|     3|       0|\n",
            "|     3|       0|\n",
            "|     3|       0|\n",
            "|     2|       1|\n",
            "|     3|       0|\n",
            "|     2|       1|\n",
            "|     3|       0|\n",
            "|     3|       1|\n",
            "+------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Vectorizamos todas las columnas numéricas en una sola columna de features.\n",
        "# La API de MLlib requiere que usemos este formato.\n",
        "feature_cols = raw_df.columns[:-1]  # Tomamos todas las columnas.\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "transformed_data = assembler.transform(raw_df)\n"
      ],
      "metadata": {
        "id": "rCOiiooEZqr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, test_data) = transformed_data.randomSplit([0.7, 0.3])"
      ],
      "metadata": {
        "id": "kMpFWbtVam_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
        "model = dt.fit(train_data)"
      ],
      "metadata": {
        "id": "28PxJN5XaspA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Survived\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy = {round(accuracy * 100, 2)} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6dZW1YUayqQ",
        "outputId": "9df46028-6e76-408c-b4f0-3149abfb34e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 65.64 %\n"
          ]
        }
      ]
    }
  ]
}
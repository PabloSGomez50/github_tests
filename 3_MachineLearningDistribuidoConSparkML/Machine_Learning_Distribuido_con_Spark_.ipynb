{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning con Spark\n",
        "\n",
        "[**MLlib**](https://spark.apache.org/docs/latest/ml-guide.html) es la biblioteca de machine learning (ML) de Spark. Permite realizar tareas de ML de forma escalable y fácil.\n",
        "\n",
        "A un alto nivel, proporciona herramientas como:\n",
        "\n",
        "* Algoritmos de ML: Como serian clasificación, regresión, clustering y filtrado colaborativo\n",
        "* Featurization: extracción de features, transformación, reducción de dimensionalidad y selección de features\n",
        "* Pipelines: herramientas para construir, evaluar y ajustar ML Pipelines\n",
        "* Persistencia: guardar y cargar algoritmos, modelos y Pipelines\n",
        "* Utilidades: álgebra lineal, estadística, manejo de datos, etc.\n"
      ],
      "metadata": {
        "id": "zWXecv5L3UHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trabajando con MLlib es vamos a usar objetos de los siquientes tipos:\n",
        "\n",
        "* `DataFrame`: MLlib usa DataFrames de Spark SQL como un dataset de datos de ML, que puede contener una variedad de tipos de datos. Por ejemplo, un DataFrame podría tener diferentes columnas que almacenan texto, vectores de características, etiquetas verdaderas y predicciones.\n",
        "\n",
        "* `Transformer`: Un `Transformer` es un algoritmo que puede transformar un DataFrame en otro. Por ejemplo, un modelo de ML es un `Transformer` que transforma un DataFrame con features a uno con predicciones.\n",
        "\n",
        "* `Estimator`: Un `Estimator` es un algoritmo que se puede ajustar (fittear) en un DataFrame para producir un `Transformer`. Por ejemplo, un algoritmo de ML es un `Estimator` que se entrena con un DataFrame y produce un modelo.\n",
        "\n",
        "* `Pipeline`: Un `Pipeline` encadena varios transformadores y estimadores para definir un workflow de ML."
      ],
      "metadata": {
        "id": "AN-sWxeqP3xV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workflow Habitual de ML\n",
        "\n",
        "En el siguiente diagrama podemos ver como es el flujo de trabajo habitual de un proyecto de ML:"
      ],
      "metadata": {
        "id": "CwqM6w73-_QK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://storage.googleapis.com/humai-datasets/imagenes/big_data_pyspark/3_MachineLearningDistribuidoConSparkML/35c2a0c9-ebac-40c8-ad0f-918af209c3a9_f366040e-1ec5-4a7b-9d0e-277f00c308e1.webp' />"
      ],
      "metadata": {
        "id": "mIDOQPp8-zIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver las etapas presentes tienen un mapeo directo con las features que nos ofrece MLlib.\n"
      ],
      "metadata": {
        "id": "tvinf3A6_LWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "---\n",
        "\n",
        "Primero installamos PySpark."
      ],
      "metadata": {
        "id": "S35W72m-5Cc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark -q"
      ],
      "metadata": {
        "id": "x-W8Iq145GMZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b47b6a-99c9-4908-ea1c-5ff7a9ab0106"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos el cliente de Spark."
      ],
      "metadata": {
        "id": "-xwVfejd7A1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "RZOYvPBT65zn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de Datos"
      ],
      "metadata": {
        "id": "aY_Mov3k8w8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ilustrar los temas vamos a utilizar a modo de ejemplo el dataset [Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) el cual contiene datos de mediciones de caracteristicas de plantes y es facil de analizar.\n",
        "En este dataset cada linea contiene datos de un individuo en particular (columnas `sepal_length`, `sepal_width`, `petal_length` y `petal_width`) y la variedad a la cual pertenece (columna `class`).\n",
        "\n",
        "Ahora si vamos a bajar el dataset de prueba y lo cargamos en un DataFrame.\n"
      ],
      "metadata": {
        "id": "Bbwk3SEB7avf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://gist.github.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvaiJ7MT7FfV",
        "outputId": "ae5d0ac0-7711-491e-8587-b0d233477fc9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-04 23:45:51--  https://gist.github.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\n",
            "Resolving gist.github.com (gist.github.com)... 140.82.114.3\n",
            "Connecting to gist.github.com (gist.github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv [following]\n",
            "--2023-08-04 23:45:51--  https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3975 (3.9K) [text/plain]\n",
            "Saving to: ‘iris.csv’\n",
            "\n",
            "iris.csv            100%[===================>]   3.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-08-04 23:45:51 (66.8 MB/s) - ‘iris.csv’ saved [3975/3975]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "raw_df = spark.createDataFrame(pd.read_csv('iris.csv', skiprows=1, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']))\n",
        "raw_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwhub4-n7ZeB",
        "outputId": "03435b66-5661-4341-f36d-19915dfe33c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width| class|\n",
            "+------------+-----------+------------+-----------+------+\n",
            "|         5.1|        3.5|         1.4|        0.2|Setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2|Setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2|Setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2|Setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2|Setosa|\n",
            "|         5.4|        3.9|         1.7|        0.4|Setosa|\n",
            "|         4.6|        3.4|         1.4|        0.3|Setosa|\n",
            "|         5.0|        3.4|         1.5|        0.2|Setosa|\n",
            "|         4.4|        2.9|         1.4|        0.2|Setosa|\n",
            "|         4.9|        3.1|         1.5|        0.1|Setosa|\n",
            "|         5.4|        3.7|         1.5|        0.2|Setosa|\n",
            "|         4.8|        3.4|         1.6|        0.2|Setosa|\n",
            "|         4.8|        3.0|         1.4|        0.1|Setosa|\n",
            "|         4.3|        3.0|         1.1|        0.1|Setosa|\n",
            "|         5.8|        4.0|         1.2|        0.2|Setosa|\n",
            "|         5.7|        4.4|         1.5|        0.4|Setosa|\n",
            "|         5.4|        3.9|         1.3|        0.4|Setosa|\n",
            "|         5.1|        3.5|         1.4|        0.3|Setosa|\n",
            "|         5.7|        3.8|         1.7|        0.3|Setosa|\n",
            "|         5.1|        3.8|         1.5|        0.3|Setosa|\n",
            "+------------+-----------+------------+-----------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Vectorizamos todas las columnas numéricas en una sola columna de features.\n",
        "# La API de MLlib requiere que usemos este formato.\n",
        "feature_cols = raw_df.columns[:-1]  # Tomamos todas las columnas.\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "data = assembler.transform(raw_df)\n"
      ],
      "metadata": {
        "id": "v0H5FBrB87Cb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estadistica Descriptiva\n",
        "---\n",
        "\n",
        "Antes de empezar a trabajar es recomendable estudiar los datos que tenemos para entenderlos mejor, verificar si necesitan limpieza y evaluar estrategias para resolver el problema que nos interese.\n",
        "\n",
        "Para esto vamos a usar el objeto [Summarizer](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.stat.Summarizer.html) el cual nos generara métricas utiles para explorar los datos.\n",
        "\n",
        "La salida del metodo `summary` es un DataFrame de tuplas correspondientes a cada campo y a cada valor estadistico que hayamos requerido. Para el campo `count` solo devuelve un solo resultado debido a que se trata del conteo total de filas."
      ],
      "metadata": {
        "id": "ksR97Xge5tgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import Summarizer\n",
        "\n",
        "summarizer = Summarizer.metrics(\n",
        "  \"count\",\n",
        "  \"mean\",\n",
        "  \"sum\",\n",
        "  \"variance\",\n",
        "  \"std\",\n",
        "  \"max\",\n",
        "  \"min\",\n",
        ")\n",
        "\n",
        "# Computamos las metricas\n",
        "rv = data.select(summarizer.summary(data.features)).show(truncate=False)\n",
        "rv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT0dWgKi78hu",
        "outputId": "cf782e42-99e9-461d-ca14-e234f053f24d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|aggregate_metrics(features, 1.0)                                                                                                                                                                                                                                                                                                           |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|{150, [5.843333333333333,3.0573333333333315,3.758,1.1993333333333336], [876.4999999999999,458.59999999999974,563.7,179.90000000000003], [0.6856935123042518,0.1899794183445187,3.1162778523489933,0.5810062639821031], [0.8280661279778637,0.4358662849366979,1.7652982332594664,0.7622376689603467], [7.9,4.4,6.9,2.5], [4.3,2.0,1.0,0.1]}|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro a Clasificacion con MLlib\n",
        "\n",
        "Como se menciono anteriormente MLlib ofrece una variedad de algoritmos que podemos utilizar para distintos problemas de Machine Learning:\n",
        "\n",
        "* Clasificación: Prediccion de clases discretas.\n",
        "* Regresión: Prediccion de valores continuos. discretas.\n",
        "* Clustering: Identificar grupos de elementos similares.\n",
        "* Filtrado Colaborativo: Metodo comunmente usando para generar recomendaciones. Apunta a rellenar valores desconocidos a partir de otras obsevaciones. Una aplicacionde esto es predecir el rating que un usuario le daria a una pelicula de lo cual se puede usar para recomendarsela.\n",
        "* Frequent Pattern Mining: Identificacion de patrones recurrentes en secuencias o grupos.\n",
        "\n",
        "\n",
        "A modo de ejemplo vamos a considerar el problema de clasificacion de los individuos de dataset segun su variedad."
      ],
      "metadata": {
        "id": "rd4GODFS5xA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformacion de Datos\n",
        "\n",
        "MLlib nos ofrece una [amplia variedad de transformaciones de datos](https://spark.apache.org/docs/latest/ml-features.html) entre las que podemos mencionar:\n",
        "\n",
        "* [MinMaxScaler](https://spark.apache.org/docs/latest/ml-features.html#minmaxscaler): Normaliza las escalas de los features.\n",
        "* [FeatureHasher](https://spark.apache.org/docs/latest/ml-features.html#featurehasher): Para trabajar con datos categoricos de alta cardinalidad.\n",
        "* [Tokenizer](https://spark.apache.org/docs/latest/ml-features.html#tokenizer): Para partir textos en unidades mas chicas.\n",
        "* [Word2Vec](https://spark.apache.org/docs/latest/ml-features.html#word2vec): Nos permite convertir palabras en vectores numericos que preservan su significado semantico.\n",
        "\n"
      ],
      "metadata": {
        "id": "v7EgG8D4FXPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuando con el ejemplo podemos ver en la metrica `mean` computada anteriormente el promedio de los las distintas variables es diferente. Esto puede ser un problema para algunos algoritmos de ML. Para resolverlo vamos a llevar todas las variables a valores en 0 y 1."
      ],
      "metadata": {
        "id": "7lCF-Qe6LhzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "\n",
        "# Ajustamos el modelo de MinMaxScalerModel a nuestros datos\n",
        "scalerModel = scaler.fit(data)\n",
        "\n",
        "# Rescalamos cada feature al rango [0, 1].\n",
        "transformed_data = scalerModel.transform(data)\n",
        "print(\"Features escaladas al rango: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
        "transformed_data.select(\"features\", \"scaledFeatures\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxEG3S48HH3d",
        "outputId": "1f670de0-bf73-4acd-ebab-be301f3b3b0e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features escaladas al rango: [0.000000, 1.000000]\n",
            "+-----------------+--------------------+\n",
            "|         features|      scaledFeatures|\n",
            "+-----------------+--------------------+\n",
            "|[5.1,3.5,1.4,0.2]|[0.22222222222222...|\n",
            "|[4.9,3.0,1.4,0.2]|[0.16666666666666...|\n",
            "|[4.7,3.2,1.3,0.2]|[0.11111111111111...|\n",
            "|[4.6,3.1,1.5,0.2]|[0.08333333333333...|\n",
            "|[5.0,3.6,1.4,0.2]|[0.19444444444444...|\n",
            "|[5.4,3.9,1.7,0.4]|[0.30555555555555...|\n",
            "|[4.6,3.4,1.4,0.3]|[0.08333333333333...|\n",
            "|[5.0,3.4,1.5,0.2]|[0.19444444444444...|\n",
            "|[4.4,2.9,1.4,0.2]|[0.02777777777777...|\n",
            "|[4.9,3.1,1.5,0.1]|[0.16666666666666...|\n",
            "|[5.4,3.7,1.5,0.2]|[0.30555555555555...|\n",
            "|[4.8,3.4,1.6,0.2]|[0.13888888888888...|\n",
            "|[4.8,3.0,1.4,0.1]|[0.13888888888888...|\n",
            "|[4.3,3.0,1.1,0.1]|[0.0,0.4166666666...|\n",
            "|[5.8,4.0,1.2,0.2]|[0.41666666666666...|\n",
            "|[5.7,4.4,1.5,0.4]|[0.38888888888888...|\n",
            "|[5.4,3.9,1.3,0.4]|[0.30555555555555...|\n",
            "|[5.1,3.5,1.4,0.3]|[0.22222222222222...|\n",
            "|[5.7,3.8,1.7,0.3]|[0.38888888888888...|\n",
            "|[5.1,3.8,1.5,0.3]|[0.22222222222222...|\n",
            "+-----------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aparte de eso tambien necesitamos convertir la clase que es una columna textual a un indice numerico."
      ],
      "metadata": {
        "id": "55oYDqc5H59A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Convertimos las feature textuales en indices.\n",
        "label_indexer = StringIndexer(inputCol='class', outputCol='label').fit(transformed_data)\n",
        "transformed_data = label_indexer.transform(transformed_data)"
      ],
      "metadata": {
        "id": "dophyurLH6M9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9cCAndnIb5D",
        "outputId": "e28667b4-da63-4e90-a5f8-4f22a0178a8e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+------+-----------------+--------------------+-----+\n",
            "|sepal_length|sepal_width|petal_length|petal_width| class|         features|      scaledFeatures|label|\n",
            "+------------+-----------+------------+-----------+------+-----------------+--------------------+-----+\n",
            "|         5.1|        3.5|         1.4|        0.2|Setosa|[5.1,3.5,1.4,0.2]|[0.22222222222222...|  0.0|\n",
            "|         4.9|        3.0|         1.4|        0.2|Setosa|[4.9,3.0,1.4,0.2]|[0.16666666666666...|  0.0|\n",
            "|         4.7|        3.2|         1.3|        0.2|Setosa|[4.7,3.2,1.3,0.2]|[0.11111111111111...|  0.0|\n",
            "|         4.6|        3.1|         1.5|        0.2|Setosa|[4.6,3.1,1.5,0.2]|[0.08333333333333...|  0.0|\n",
            "|         5.0|        3.6|         1.4|        0.2|Setosa|[5.0,3.6,1.4,0.2]|[0.19444444444444...|  0.0|\n",
            "|         5.4|        3.9|         1.7|        0.4|Setosa|[5.4,3.9,1.7,0.4]|[0.30555555555555...|  0.0|\n",
            "|         4.6|        3.4|         1.4|        0.3|Setosa|[4.6,3.4,1.4,0.3]|[0.08333333333333...|  0.0|\n",
            "|         5.0|        3.4|         1.5|        0.2|Setosa|[5.0,3.4,1.5,0.2]|[0.19444444444444...|  0.0|\n",
            "|         4.4|        2.9|         1.4|        0.2|Setosa|[4.4,2.9,1.4,0.2]|[0.02777777777777...|  0.0|\n",
            "|         4.9|        3.1|         1.5|        0.1|Setosa|[4.9,3.1,1.5,0.1]|[0.16666666666666...|  0.0|\n",
            "|         5.4|        3.7|         1.5|        0.2|Setosa|[5.4,3.7,1.5,0.2]|[0.30555555555555...|  0.0|\n",
            "|         4.8|        3.4|         1.6|        0.2|Setosa|[4.8,3.4,1.6,0.2]|[0.13888888888888...|  0.0|\n",
            "|         4.8|        3.0|         1.4|        0.1|Setosa|[4.8,3.0,1.4,0.1]|[0.13888888888888...|  0.0|\n",
            "|         4.3|        3.0|         1.1|        0.1|Setosa|[4.3,3.0,1.1,0.1]|[0.0,0.4166666666...|  0.0|\n",
            "|         5.8|        4.0|         1.2|        0.2|Setosa|[5.8,4.0,1.2,0.2]|[0.41666666666666...|  0.0|\n",
            "|         5.7|        4.4|         1.5|        0.4|Setosa|[5.7,4.4,1.5,0.4]|[0.38888888888888...|  0.0|\n",
            "|         5.4|        3.9|         1.3|        0.4|Setosa|[5.4,3.9,1.3,0.4]|[0.30555555555555...|  0.0|\n",
            "|         5.1|        3.5|         1.4|        0.3|Setosa|[5.1,3.5,1.4,0.3]|[0.22222222222222...|  0.0|\n",
            "|         5.7|        3.8|         1.7|        0.3|Setosa|[5.7,3.8,1.7,0.3]|[0.38888888888888...|  0.0|\n",
            "|         5.1|        3.8|         1.5|        0.3|Setosa|[5.1,3.8,1.5,0.3]|[0.22222222222222...|  0.0|\n",
            "+------------+-----------+------------+-----------+------+-----------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split de Datos\n",
        "\n",
        "Para poder evaluar correctamente el modelo tenemos que entrenarlo sobre un dataset y luego evaluarlo contra otro. Con esto vamos a poder determinar si el modelo en lugar de aprender de los datos los memorizo (lo que se llama habitualmente overfitting).\n",
        "\n"
      ],
      "metadata": {
        "id": "dxwwoBtnI83A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, test_data) = transformed_data.randomSplit([0.7, 0.3])"
      ],
      "metadata": {
        "id": "VITHI4OXJIq9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento del Modelo\n",
        "\n",
        "Ahora que tenemos data preparada vamos a entrenar un modelo de arbol de decision que nos permitira clasificar cada fila segun su clase."
      ],
      "metadata": {
        "id": "7o-s7eD9FbPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "model = dt.fit(train_data)\n"
      ],
      "metadata": {
        "id": "3jVVKkSpHooc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluacion de performance"
      ],
      "metadata": {
        "id": "dRLtaxqT5_oF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos las predicciones del modelo.\n",
        "predictions = model.transform(test_data)\n",
        "predictions.select(\"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcFxHmp9Jojv",
        "outputId": "7f2748d0-3ee0-4d08-b185-576210292f03"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|prediction|\n",
            "+----------+\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       1.0|\n",
            "|       1.0|\n",
            "|       0.0|\n",
            "|       1.0|\n",
            "|       0.0|\n",
            "|       1.0|\n",
            "|       1.0|\n",
            "|       1.0|\n",
            "|       0.0|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora con las predicciones en el dataset de test vamos obtener el accuracy del modelo. Esto es el porcentaje de veces que el modelo hace una prediccion correcta sobre de clase sobre datos no vistos."
      ],
      "metadata": {
        "id": "RA6h6CjlKZMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy = {round(accuracy * 100, 2)} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS3F1UuBKFNh",
        "outputId": "00cefd71-ef54-4089-ec9a-ddffdf05c96b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 97.78 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluacion de Performance: Cross Validation\n",
        "\n",
        "Cuando evaluamos la performance de un modelo de ML usando una particion aleatoria de nuestro dataset en train y test puede ocurrir que obtengamos un score distinto de si tomamos otra particion. Esto es un problema particularmente marcado en datasets de tamaño reducido y alta variabilidad.\n",
        "\n",
        "Para solucionar este problema se lleva a cabo un proceso llamado cross-validation. Aqui se divide en dataset en particiones igual que se usan alternativamente para entrenar y evaluar. Cada corrida produce un score que los cuales se promedian para generar el score final del modelo."
      ],
      "metadata": {
        "id": "loxrCkwNbtlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://storage.googleapis.com/humai-datasets/imagenes/big_data_pyspark/3_MachineLearningDistribuidoConSparkML/ea1fdc34-aa90-41e6-9491-9add3653e2eb_image.png' />"
      ],
      "metadata": {
        "id": "4s75dsUEdWaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "paramGrid = ParamGridBuilder().build()\n",
        "\n",
        "crossval = CrossValidator(\n",
        "    estimator=dt,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=3\n",
        ")  # Usamos 3 folds pero en la practica se usan mas\n",
        "\n",
        "# Corremos cross-validation.\n",
        "cvModel = crossval.fit(transformed_data)\n",
        "print(f\"Average accuracy for all folds = {round(cvModel.avgMetrics[0] * 100, 2)} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofKjDxcyeCFc",
        "outputId": "efd62f15-c476-443b-9ac8-403cc374f16e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy for all folds = 93.65 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tunneo de hiperparametros\n",
        "\n",
        "Una caracteristica de los algoritmos de ML que producen los modelos es que tienen parametros que controlan la ejecucion de los mismos pero no son aprendidos de la data.\n",
        "Estos parametros pueden tener gran impacto en los datos por lo cual es deseable encontrar su mejor configuracion posible, este proceso se llama hyperparameter tuning. Para hacer esto se realizan procesos de busqueda que pueden tener distintos niveles de complejidad, en el caso de MLlib se utiliza una forma basica que consiste en probar combinaciones secuencialmente.\n",
        "\n"
      ],
      "metadata": {
        "id": "MX3Fk3AWgZKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import TrainValidationSplit\n",
        "\n",
        "paramGrid = ParamGridBuilder()\\\n",
        "    .addGrid(dt.maxDepth, [1, 5, 10]) \\\n",
        "    .build()\n",
        "\n",
        "tvs = TrainValidationSplit(\n",
        "    estimator=dt,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    trainRatio=.7,\n",
        ")\n",
        "\n",
        "# Corremos cross-validation.\n",
        "model_tvs = tvs.fit(transformed_data)\n",
        "print(f\"Accuracy de cada modelo entrenado = {model_tvs.validationMetrics}\")\n",
        "\n",
        "# En este punto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_HqvDxjjdhk",
        "outputId": "ab34ccea-71b9-4f85-b3e9-5b79c7913cd8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy de cada modelo entrenado = [0.5952380952380952, 0.9285714285714286, 0.9285714285714286]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "print(f\"Accuracy of each trained model = {model_tvs.params}\")\n",
        "print(f\"Mejor modelo entrenado: {model_tvs.bestModel}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuG9y_q6kohK",
        "outputId": "a3f7f3a1-7814-4d12-f99f-fa12f468e476"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of each trained model = [Param(parent='TrainValidationSplitModel_ca4b4d83c2d9', name='estimator', doc='estimator to be cross-validated'), Param(parent='TrainValidationSplitModel_ca4b4d83c2d9', name='estimatorParamMaps', doc='estimator param maps'), Param(parent='TrainValidationSplitModel_ca4b4d83c2d9', name='evaluator', doc='evaluator used to select hyper-parameters that maximize the validator metric'), Param(parent='TrainValidationSplitModel_ca4b4d83c2d9', name='seed', doc='random seed.'), Param(parent='TrainValidationSplitModel_ca4b4d83c2d9', name='trainRatio', doc='Param for ratio between train and     validation data. Must be between 0 and 1.')]\n",
            "Mejor modelo entrenado: DecisionTreeClassificationModel: uid=DecisionTreeClassifier_77e35eada83d, depth=5, numNodes=17, numClasses=3, numFeatures=4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persistiendo Modelos\n",
        "\n",
        "MLlib tambien nos permite guardar modelos para luego usarlos nuevamente.\n"
      ],
      "metadata": {
        "id": "yl9pDPO_Mg-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!if [ -d \"/tmp/model\" ]; then rm -r /tmp/model; fi"
      ],
      "metadata": {
        "id": "8XUnKGXzX1EW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvamos el modelo\n",
        "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
        "model.save(\"/tmp/model\")"
      ],
      "metadata": {
        "id": "iKqfgby1MW2P"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -Rl /tmp/model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izWFF1kSMaYG",
        "outputId": "245e258c-fdd1-4708-9870-d4faa5ff26e4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/model:\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Aug  4 23:46 data\n",
            "drwxr-xr-x 2 root root 4096 Aug  4 23:46 metadata\n",
            "\n",
            "/tmp/model/data:\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 4057 Aug  4 23:46 part-00000-3547bed7-5335-4a28-9436-07cc07ccffef-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root    0 Aug  4 23:46 _SUCCESS\n",
            "\n",
            "/tmp/model/metadata:\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 642 Aug  4 23:46 part-00000\n",
            "-rw-r--r-- 1 root root   0 Aug  4 23:46 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lo cargamos y comprobamos que funciona correctamente.\n",
        "model_loaded = DecisionTreeClassificationModel.load(\"/tmp/model\")\n",
        "predictions = model_loaded.transform(test_data)\n",
        "predictions.select(\"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XXPQkADNgnU",
        "outputId": "142c6143-df5c-4099-feff-5e5a0072b540"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|prediction|\n",
            "+----------+\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       0.0|\n",
            "|       1.0|\n",
            "|       1.0|\n",
            "|       0.0|\n",
            "|       1.0|\n",
            "|       0.0|\n",
            "|       1.0|\n",
            "|       1.0|\n",
            "|       1.0|\n",
            "|       0.0|\n",
            "+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelines\n",
        "\n",
        "La funcionalidad de [ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html) proporciona un conjunto uniforme de APIs de alto nivel que ayudan a crear y fittear conjuntos de operaciones de manera sencilla.\n",
        "\n",
        "Para cerrar vamos ver como encadenar las operaciones vistas hasta ahora en el ejemplo en un pipeline.\n"
      ],
      "metadata": {
        "id": "6yaMwqfiT8BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Cargamos los datos\n",
        "raw_df = spark.createDataFrame(\n",
        "    pd.read_csv(\n",
        "        'iris.csv',\n",
        "        skiprows=1,\n",
        "        names=[\n",
        "            'sepal_length',\n",
        "            'sepal_width',\n",
        "            'petal_length',\n",
        "            'petal_width',\n",
        "            'class'\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Generamos un split de datos al comienzo.\n",
        "(train_raw_df, test_raw_df) = raw_df.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Creamos las mismas transformacion de los pasos anteriores de antemano.\n",
        "feature_cols = raw_df.columns[:-1]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "label_indexer = StringIndexer(inputCol='class', outputCol='label')\n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "\n",
        "# Creamos el Pipeline que encadena las transformaciones.\n",
        "pipeline = Pipeline(stages=[assembler, scaler, label_indexer, dt])\n",
        "\n",
        "# Fitteamos todos los pasos del pipeline.\n",
        "model = pipeline.fit(train_raw_df)\n",
        "\n",
        "# Generamos las predicciones\n",
        "predictions = model.transform(test_raw_df)\n",
        "\n",
        "# Evaluamos la performance del modelo.\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"accuracy\"\n",
        ")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy = {round(accuracy * 100, 2)} %\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_yDuNG5T8Sl",
        "outputId": "4251165a-a30c-4eb7-bbd5-d0fd77ea3c31"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 93.02 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Referencias\n",
        "\n",
        " * https://spark.apache.org/docs/latest/ml-guide.html"
      ],
      "metadata": {
        "id": "N3iF069I4ley"
      }
    }
  ]
}
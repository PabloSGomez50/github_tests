{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13eb98b5",
      "metadata": {
        "id": "13eb98b5"
      },
      "source": [
        "# Ejercicios PySpark clase 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.2.0\n",
        "!wget https://github.com/openscoring/openscoring/releases/download/2.1.0/openscoring-server-executable-2.1.0.jar\n",
        "!wget https://downloads.apache.org/kafka/3.4.1/kafka_2.12-3.4.1.tgz\n",
        "!tar -xzf kafka_2.12-3.4.1.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZMkjTCbIy9B",
        "outputId": "a1750e4e-e0d7-45b8-95f5-2a42755ed02f"
      },
      "id": "gZMkjTCbIy9B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.2.0 in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.2.0) (0.10.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.2.0\n",
        "!wget https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\n",
        "!wget https://github.com/openscoring/openscoring/releases/download/2.1.0/openscoring-server-executable-2.1.0.jar\n",
        "!wget https://downloads.apache.org/kafka/3.4.1/kafka_2.12-3.4.1.tgz\n",
        "!tar -xzf kafka_2.12-3.4.1.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ign-uZaBSnOp",
        "outputId": "c1e958ea-d158-48cc-f0cb-925a9000d7f0"
      },
      "id": "ign-uZaBSnOp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.2.0\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.2 (from pyspark==3.2.0)\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805893 sha256=dffc03443345a728f7610d0bab2e70ef39d31861b7e3afb074c0c13406469da9\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/97/d3/8b6d964c8700e4fbb561c71638a92ec55dac9be51eb5fea86d\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n",
            "--2023-10-02 21:27:07--  https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3975 (3.9K) [text/plain]\n",
            "Saving to: ‘iris.csv’\n",
            "\n",
            "iris.csv            100%[===================>]   3.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-10-02 21:27:07 (70.6 MB/s) - ‘iris.csv’ saved [3975/3975]\n",
            "\n",
            "--2023-10-02 21:27:07--  https://github.com/openscoring/openscoring/releases/download/2.1.0/openscoring-server-executable-2.1.0.jar\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/9178484/e5dac4d7-9d82-4026-9aad-b7148765e61e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231002%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231002T212707Z&X-Amz-Expires=300&X-Amz-Signature=15e4e738c769aa555989c12163da2dd1178c4e4e7dd666747b0a71fa59d1d4c7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=9178484&response-content-disposition=attachment%3B%20filename%3Dopenscoring-server-executable-2.1.0.jar&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-10-02 21:27:07--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/9178484/e5dac4d7-9d82-4026-9aad-b7148765e61e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231002%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231002T212707Z&X-Amz-Expires=300&X-Amz-Signature=15e4e738c769aa555989c12163da2dd1178c4e4e7dd666747b0a71fa59d1d4c7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=9178484&response-content-disposition=attachment%3B%20filename%3Dopenscoring-server-executable-2.1.0.jar&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25110251 (24M) [application/octet-stream]\n",
            "Saving to: ‘openscoring-server-executable-2.1.0.jar’\n",
            "\n",
            "openscoring-server- 100%[===================>]  23.95M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-10-02 21:27:07 (322 MB/s) - ‘openscoring-server-executable-2.1.0.jar’ saved [25110251/25110251]\n",
            "\n",
            "--2023-10-02 21:27:07--  https://downloads.apache.org/kafka/3.4.1/kafka_2.12-3.4.1.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.95.219, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 106809260 (102M) [application/x-gzip]\n",
            "Saving to: ‘kafka_2.12-3.4.1.tgz’\n",
            "\n",
            "kafka_2.12-3.4.1.tg 100%[===================>] 101.86M  24.6MB/s    in 4.3s    \n",
            "\n",
            "2023-10-02 21:27:12 (23.5 MB/s) - ‘kafka_2.12-3.4.1.tgz’ saved [106809260/106809260]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab62899b",
      "metadata": {
        "id": "ab62899b"
      },
      "source": [
        "## Operaciones con DataFrames en PySpark - Análisis de Videojuegos\n",
        "Se proporciona un dataset de videojuegos llamado 'videogames.csv'. Realiza las siguientes operaciones utilizando PySpark:\n",
        "- Carga el dataset como un DataFrame.\n",
        "- Filtra los videojuegos que no pertenezcan a la plataforma \"PC\".\n",
        "- Cuenta la cantidad de videojuegos para cada género.\n",
        "- Muestra el top 5 de los géneros con más videojuegos.\n",
        "- Escribe el resultado en un archivo CSV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0573b655",
      "metadata": {
        "id": "0573b655"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "def top_5_genres(spark):\n",
        "    # Read the dataset\n",
        "    df = spark.read.csv('videogames.csv', header=True)\n",
        "\n",
        "    # Filter the videogames for platform 'PC'\n",
        "    df = df.filter(df.Platform == 'PC')\n",
        "\n",
        "    # Count the video games by genre\n",
        "    result = df.groupBy('Genre').agg(count('*').alias('total_games'))\n",
        "\n",
        "    # Get the top 5 genres\n",
        "    result = result.sort('total_games', ascending=False).limit(5)\n",
        "\n",
        "    # Write results to CSV\n",
        "    result.write.csv('top_5_genres.csv', mode='overwrite')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3f32eaa",
      "metadata": {
        "id": "f3f32eaa"
      },
      "source": [
        "## Análisis de tendencias en tweets en tiempo real\n",
        "Se reciben trasmisiones de tweets por un sistema Kafka con el topic 'tweets'. Utilizando PySpark Streaming, conectarse al sistema Kafka y realizar las siguientes operaciones:\n",
        "- Leer los tweets en tiempo real.\n",
        "- Filtrar los tweets que contengan ciertas palabras clave, por ejemplo, \"covid\", \"vaccine\", \"pandemic\".\n",
        "- Contar la cantidad de tweets por cada palabra clave.\n",
        "- Imprimir el conteo actualizado en la consola.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./kafka_2.12-3.4.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.12-3.4.1/config/zookeeper.properties\n",
        "!./kafka_2.12-3.4.1/bin/kafka-server-start.sh -daemon ./kafka_2.12-3.4.1/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 20\n",
        "!ps -ef | grep kafka\n",
        "# iniciando el tópico iris con replicación 1 y 1 partición\n",
        "!./kafka_2.12-3.4.1/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic tweets\n",
        "!./kafka_2.12-3.4.1/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic tweets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwN-6dChSvb0",
        "outputId": "7b614603-4a14-4db5-eea5-6331b2c26f1c"
      },
      "id": "ZwN-6dChSvb0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for 10 secs until kafka and zookeeper services are up and running\n",
            "root        1639       1 11 21:27 ?        00:00:02 java -Xmx512M -Xms512M -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Xlog:gc*:file=/content/kafka_2.12-3.4.1/bin/../logs/zookeeper-gc.log:time,tags:filecount=10,filesize=100M -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/content/kafka_2.12-3.4.1/bin/../logs -Dlog4j.configuration=file:./kafka_2.12-3.4.1/bin/../config/log4j.properties -cp /content/kafka_2.12-3.4.1/bin/../libs/activation-1.1.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/aopalliance-repackaged-2.6.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/argparse4j-0.7.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/audience-annotations-0.13.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/commons-cli-1.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/commons-lang3-3.8.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-api-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-basic-auth-extension-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-json-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-mirror-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-mirror-client-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-runtime-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-transforms-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/hk2-api-2.6.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/hk2-locator-2.6.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/hk2-utils-2.6.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-annotations-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-core-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-databind-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-dataformat-csv-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-datatype-jdk8-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-jaxrs-base-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-jaxrs-json-provider-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-module-jaxb-annotations-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-module-scala_2.12-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jakarta.activation-api-1.2.2.jar:/content/kafka_2.12-3.4.1/bin/../libs/jakarta.annotation-api-1.3.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jakarta.inject-2.6.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jakarta.validation-api-2.0.2.jar:/content/kafka_2.12-3.4.1/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/content/kafka_2.12-3.4.1/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/content/kafka_2.12-3.4.1/bin/../libs/javassist-3.29.2-GA.jar:/content/kafka_2.12-3.4.1/bin/../libs/javax.activation-api-1.2.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/javax.annotation-api-1.3.2.jar:/content/kafka_2.12-3.4.1/bin/../libs/javax.servlet-api-3.1.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/javax.ws.rs-api-2.1.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jaxb-api-2.3.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jersey-client-2.39.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jersey-common-2.39.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jersey-container-servlet-2.39.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jersey-hk2-2.39.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jersey-server-2.39.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-client-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-continuation-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-http-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-io-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-security-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-server-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-servlet-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-servlets-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-util-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-util-ajax-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jline-3.22.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/jopt-simple-5.0.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/jose4j-0.9.3.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka_2.12-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-clients-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-group-coordinator-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-log4j-appender-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-metadata-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-raft-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-server-common-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-shell-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-storage-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-storage-api-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-streams-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-streams-examples-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-streams-scala_2.12-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-streams-test-utils-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-tools-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/lz4-java-1.8.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/maven-artifact-3.8.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/metrics-core-2.2.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/metrics-core-4.1.12.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-buffer-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-codec-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-common-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-handler-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-resolver-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-transport-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-transport-classes-epoll-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-transport-native-epoll-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-transport-native-unix-common-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/osgi-resource-locator-1.0.3.jar:/content/kafka_2.12-3.4.1/bin/../libs/paranamer-2.8.jar:/content/kafka_2.12-3.4.1/bin/../libs/plexus-utils-3.3.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/reflections-0.9.12.jar:/content/kafka_2.12-3.4.1/bin/../libs/reload4j-1.2.25.jar:/content/kafka_2.12-3.4.1/bin/../libs/rocksdbjni-7.1.2.jar:/content/kafka_2.12-3.4.1/bin/../libs/scala-collection-compat_2.12-2.10.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/scala-java8-compat_2.12-1.0.2.jar:/content/kafka_2.12-3.4.1/bin/../libs/scala-library-2.12.15.jar:/content/kafka_2.12-3.4.1/bin/../libs/scala-logging_2.12-3.9.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/scala-reflect-2.12.15.jar:/content/kafka_2.12-3.4.1/bin/../libs/slf4j-api-1.7.36.jar:/content/kafka_2.12-3.4.1/bin/../libs/slf4j-reload4j-1.7.36.jar:/content/kafka_2.12-3.4.1/bin/../libs/snappy-java-1.1.8.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/swagger-annotations-2.2.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/trogdor-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/zookeeper-3.6.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/zookeeper-jute-3.6.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/zstd-jni-1.5.2-1.jar org.apache.zookeeper.server.quorum.QuorumPeerMain ./kafka_2.12-3.4.1/config/zookeeper.properties\n",
            "root        2014       1 41 21:27 ?        00:00:08 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Xlog:gc*:file=/content/kafka_2.12-3.4.1/bin/../logs/kafkaServer-gc.log:time,tags:filecount=10,filesize=100M -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/content/kafka_2.12-3.4.1/bin/../logs -Dlog4j.configuration=file:./kafka_2.12-3.4.1/bin/../config/log4j.properties -cp /content/kafka_2.12-3.4.1/bin/../libs/activation-1.1.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/aopalliance-repackaged-2.6.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/argparse4j-0.7.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/audience-annotations-0.13.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/commons-cli-1.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/commons-lang3-3.8.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-api-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-basic-auth-extension-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-json-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-mirror-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-mirror-client-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-runtime-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/connect-transforms-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/hk2-api-2.6.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/hk2-locator-2.6.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/hk2-utils-2.6.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-annotations-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-core-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-databind-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-dataformat-csv-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-datatype-jdk8-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-jaxrs-base-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-jaxrs-json-provider-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-module-jaxb-annotations-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jackson-module-scala_2.12-2.13.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jakarta.activation-api-1.2.2.jar:/content/kafka_2.12-3.4.1/bin/../libs/jakarta.annotation-api-1.3.5.jar:/content/kafka_2.12-3.4.1/bin/../libs/jakarta.inject-2.6.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jakarta.validation-api-2.0.2.jar:/content/kafka_2.12-3.4.1/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/content/kafka_2.12-3.4.1/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/content/kafka_2.12-3.4.1/bin/../libs/javassist-3.29.2-GA.jar:/content/kafka_2.12-3.4.1/bin/../libs/javax.activation-api-1.2.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/javax.annotation-api-1.3.2.jar:/content/kafka_2.12-3.4.1/bin/../libs/javax.servlet-api-3.1.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/javax.ws.rs-api-2.1.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jaxb-api-2.3.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jersey-client-2.39.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jersey-common-2.39.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jersey-container-servlet-2.39.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jersey-hk2-2.39.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jersey-server-2.39.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-client-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-continuation-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-http-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-io-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-security-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-server-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-servlet-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-servlets-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-util-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jetty-util-ajax-9.4.51.v20230217.jar:/content/kafka_2.12-3.4.1/bin/../libs/jline-3.22.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/jopt-simple-5.0.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/jose4j-0.9.3.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka_2.12-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-clients-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-group-coordinator-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-log4j-appender-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-metadata-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-raft-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-server-common-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-shell-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-storage-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-storage-api-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-streams-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-streams-examples-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-streams-scala_2.12-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-streams-test-utils-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/kafka-tools-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/lz4-java-1.8.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/maven-artifact-3.8.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/metrics-core-2.2.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/metrics-core-4.1.12.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-buffer-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-codec-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-common-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-handler-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-resolver-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-transport-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-transport-classes-epoll-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-transport-native-epoll-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/netty-transport-native-unix-common-4.1.92.Final.jar:/content/kafka_2.12-3.4.1/bin/../libs/osgi-resource-locator-1.0.3.jar:/content/kafka_2.12-3.4.1/bin/../libs/paranamer-2.8.jar:/content/kafka_2.12-3.4.1/bin/../libs/plexus-utils-3.3.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/reflections-0.9.12.jar:/content/kafka_2.12-3.4.1/bin/../libs/reload4j-1.2.25.jar:/content/kafka_2.12-3.4.1/bin/../libs/rocksdbjni-7.1.2.jar:/content/kafka_2.12-3.4.1/bin/../libs/scala-collection-compat_2.12-2.10.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/scala-java8-compat_2.12-1.0.2.jar:/content/kafka_2.12-3.4.1/bin/../libs/scala-library-2.12.15.jar:/content/kafka_2.12-3.4.1/bin/../libs/scala-logging_2.12-3.9.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/scala-reflect-2.12.15.jar:/content/kafka_2.12-3.4.1/bin/../libs/slf4j-api-1.7.36.jar:/content/kafka_2.12-3.4.1/bin/../libs/slf4j-reload4j-1.7.36.jar:/content/kafka_2.12-3.4.1/bin/../libs/snappy-java-1.1.8.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/swagger-annotations-2.2.0.jar:/content/kafka_2.12-3.4.1/bin/../libs/trogdor-3.4.1.jar:/content/kafka_2.12-3.4.1/bin/../libs/zookeeper-3.6.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/zookeeper-jute-3.6.4.jar:/content/kafka_2.12-3.4.1/bin/../libs/zstd-jni-1.5.2-1.jar kafka.Kafka ./kafka_2.12-3.4.1/config/server.properties\n",
            "root        2184     252  0 21:27 ?        00:00:00 /bin/bash -c ps -ef | grep kafka\n",
            "root        2186    2184  0 21:27 ?        00:00:00 grep kafka\n",
            "Created topic iris.\n",
            "Topic: iris\tTopicId: tWTqIT5FT_-JEgQJv3W0ZQ\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: \n",
            "\tTopic: iris\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cecda44f",
      "metadata": {
        "id": "cecda44f"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import split, count\n",
        "from pyspark.sql.streaming import DataStreamReader\n",
        "\n",
        "def analyze_tweets(spark):\n",
        "    # Connect to Kafka and subscribe to the tweets topic\n",
        "    tweets = (spark.readStream\n",
        "            .format(\"kafka\")\n",
        "            .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
        "            .option(\"subscribe\", \"tweets\")\n",
        "            .load())\n",
        "\n",
        "    # Extract the tweet text\n",
        "    texts = tweets.selectExpr(\"CAST(value AS STRING) as text\")\n",
        "\n",
        "    # Split the tweets and filter for keywords\n",
        "    words = texts.select(split('text', \" \").alias('word'))\n",
        "    keywords = words.filter(words.word.isin([\"covid\", \"vaccine\", \"pandemic\"]))\n",
        "\n",
        "    # Count the tweets for each keyword\n",
        "    result = keywords.groupBy('word').count()\n",
        "\n",
        "    # Write the output to console\n",
        "    query = (result.writeStream\n",
        "            .outputMode(\"complete\")\n",
        "            .format(\"console\")\n",
        "            .start())\n",
        "\n",
        "    # Wait for query to finish\n",
        "    query.awaitTermination()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08da4383",
      "metadata": {
        "id": "08da4383"
      },
      "source": [
        "## Procesamiento de imágenes en PySpark\n",
        "Utiliza el ImageSchema de PySpark para realizar las siguientes operaciones:\n",
        "- Leer un grupo de imágenes en un DataFrame.\n",
        "- Mostrar información sobre las imágenes, como el formato, altura, anchura y el canal de color.\n",
        "- Filtrar imágenes con una altura y anchura mayor a 640x480.\n",
        "- Guardar las imágenes filtradas en un archivo binario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "349fd433",
      "metadata": {
        "id": "349fd433"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.image import ImageSchema\n",
        "\n",
        "def process_images(spark):\n",
        "    # Read images using ImageSchema\n",
        "    images = ImageSchema.readImages(\"images\")\n",
        "\n",
        "    # Display image information\n",
        "    print(images.select(\"image.height\", \"image.width\",\n",
        "                        \"image.nChannels\", \"image.mode\").collect())\n",
        "\n",
        "    # Filter images based on dimensions\n",
        "    big_images = images.filter('image.height > 640 and image.width > 480')\n",
        "\n",
        "    # Save filtered images as binary files\n",
        "    (big_images.write\n",
        "    .parquet(\"big_images_parquet\", mode='overwrite'))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK3_fvXVgjB5"
      },
      "source": [
        "# Spark Streaming\n",
        "\n",
        "El objetivo del ejercicio es utilizar el `fileStream` y el `SparkContext`. Se debe tomar el ejemplo anterior, buscar en la carpeta los archivos json que se fueron generando y hacer una agrupación de count para los valores de los features para ver si hay `sepal_width` repetidos. Los resultados pueden ser escritos a otros archivos json en formato `update` o mostrados en consola.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynyDZlkUg5Sv"
      },
      "source": [
        "## Dependencias\n",
        "\n",
        "Aquí se instalan las dependencias y descargan los archivos necesarios para correr este colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT0h2lxAgiev"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark==3.2.0\n",
        "!wget https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\n",
        "!wget https://github.com/openscoring/openscoring/releases/download/2.1.0/openscoring-server-executable-2.1.0.jar\n",
        "!wget https://downloads.apache.org/kafka/3.4.1/kafka_2.12-3.4.1.tgz\n",
        "!tar -xzf kafka_2.12-3.4.1.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvR5oqlxeOvk"
      },
      "outputs": [],
      "source": [
        "!./kafka_2.12-3.4.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.12-3.4.1/config/zookeeper.properties\n",
        "!./kafka_2.12-3.4.1/bin/kafka-server-start.sh -daemon ./kafka_2.12-3.4.1/config/server.properties\n",
        "!echo \"Waiting for 10 secs until kafka and zookeeper services are up and running\"\n",
        "!sleep 20\n",
        "!ps -ef | grep kafka\n",
        "# iniciando el tópico iris con replicación 1 y 1 partición\n",
        "!./kafka_2.12-3.4.1/bin/kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic iris\n",
        "!./kafka_2.12-3.4.1/bin/kafka-topics.sh --describe --bootstrap-server 127.0.0.1:9092 --topic iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2Ay41MvixBu"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hixfwphmiyUX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "from json import loads, dumps\n",
        "\n",
        "from uuid import uuid4\n",
        "\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.sql.types import StructType, DoubleType, StringType, IntegerType\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import col, udf, from_json, to_json, struct, md5\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import (\n",
        "    MinMaxScaler,\n",
        "    VectorAssembler,\n",
        "    OneHotEncoder,\n",
        "    StringIndexer,\n",
        "    IndexToString\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owopwYT02Hk9"
      },
      "source": [
        "### Creando el cluster de Spark con las dependencias instaladas\n",
        "\n",
        "En este caso, en vez de usar archivos JAR, estamos especificando los paquetes que necesitamos y Spark se encarga de descargarlos por nosotros (si no estuvieran presentes).\n",
        "\n",
        "Adicionalmente, se crea el cluster de Spark con `local[*]` para que el cluster decida la cantidad de threads que necesita para correr el notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1stM2jzAgjqq"
      },
      "outputs": [],
      "source": [
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.kafka:kafka-clients:2.8.1 --master local[*] pyspark-shell'\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master('local[*]') \\\n",
        "    .appName(\"Spark Streaming\") \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gVrUFGP21wO"
      },
      "source": [
        "### Importando el dataset\n",
        "\n",
        "En el siguiente bloque se define el schema. En la mayoría de los casos esto no es necesario, pero como las columnas del dataset `iris.csv` tienen puntos en los nombres: `sepal.width` Spark entiende que es un `Struct` o un objeto y trata de descomponerlo. Como no puede, este falla. Lo que hacemos para solucionar esto es cambiarle el nombre agregando *backticks* (el siguiente caracter: `)\n",
        "\n",
        "En este caso, vamos a usar la función `cache()` al final de la definición del dataset. Esto sirve para mantener el dataset en memoria y que las operaciones sean mucho más rapidas. Hacemos esto ya que luego vamos a ver como se pueden usar un dataset estático y streaming en conjunto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxMtCwi6jHwt"
      },
      "outputs": [],
      "source": [
        "iris_schema = StructType().add('sepal.length', DoubleType()) \\\n",
        "  .add('sepal.width', DoubleType()) \\\n",
        "  .add('petal.length', DoubleType()) \\\n",
        "  .add('petal.width', DoubleType()) \\\n",
        "  .add('variety', StringType())\n",
        "\n",
        "# renaming columns to remove dot for better compatibility\n",
        "iris_df = spark.read.format('csv') \\\n",
        "  .schema(iris_schema) \\\n",
        "  .option('header', 'true') \\\n",
        "  .load('iris.csv') \\\n",
        "  .select(\n",
        "      col('`sepal.width`').alias('sepal_width'),\n",
        "      col('`sepal.length`').alias('sepal_length'),\n",
        "      col('`petal.width`').alias('petal_width'),\n",
        "      col('`petal.length`').alias('petal_length'),\n",
        "      col('variety')\n",
        "    ).cache()\n",
        "iris_df.show()\n",
        "iris_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqnCnCHbEgKM"
      },
      "source": [
        "## Openscoring y copia de modelos\n",
        "\n",
        "**NOTA IMPORTANTE**: en los comandos `cp` de las celdas que siguen, deben ponerse la dirección de su drive donde apunte a estos archivos. Los archivos estan disponibles en la carpeta del colab. Para conectar colab con drive, abrir los archivos (botón arriba a la izquierda que es una carpeta) y arriba de todo habrá un ícono de drive. Si se le da click se conecta y se agrega una carpeta llamada **drive** en la dirección `/content/drive`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTmTL6xZPbDY",
        "outputId": "aa8c46d4-2c1d-41ef-b620-1ff707c973cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ],
      "source": [
        "!nohup java -jar /content/openscoring-server-executable-2.1.0.jar --port 8081 &\n",
        "!sleep 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajgUPybEwVNA",
        "outputId": "6d2baa16-e3fe-4a41-a20d-a387937cf1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTA IMPORTANTE**: el path debe ser donde cada uno guardo la carpeta_index_to_class y RandomForestIris.pmml\n",
        "El link donde encuentran la carpeta index_to_class y el archivo PMML es: https://drive.google.com/drive/folders/135dwjynARvhTEAtdS2dm85aiYqLXLKzR?usp=sharing"
      ],
      "metadata": {
        "id": "QBQHiAum1EIq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkWp4uLmZJcw"
      },
      "outputs": [],
      "source": [
        "!cp -r ./drive/MyDrive/humai/spark/index_to_class ./index_to_string\n",
        "!cp -r ./drive/MyDrive/humai/spark/RandomForestIris.pmml ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaCUWqGV402n"
      },
      "source": [
        "### Data Locallity o Localidad de la Data\n",
        "\n",
        "Las personas que diseñaron Spark notaron que es más costoeficiente \"mover los cómputos\" que \"mover la data\". Es decir, es más barato ejecutar los computos donde esta la data que mover la data a donde esta el computo. Por eso, no solamente Spark es procesamiento distribuido, sino que usa un patrón crucial para su funcionamiento óptimo. Este es, tener en cuenta la **Localidad de los datos**. Esto significa que los procesamientos que se envían al cluster de Spark, deben intentar poder ser performados por las máquinas en donde la data esta y evitar el *shuffling* (que los datos de una maquina termine en otra, que vimos que es costoso).\n",
        "\n",
        "Es importante tener esto en cuenta al momento de diseñar un sistema utilizando las tecnologías vistas en este colab. Se podría pensar en una arquitectura con los modelos desplegados en la misma máquina donde esta la data, de esta manera las consultas no saldrían de esta y sería extremadamente rápido, a pesar de que fuera HTTP.\n",
        "\n",
        "Los invito a considerar diferentes opciones y conversarlas en el discord."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wUzTVy18yvY"
      },
      "outputs": [],
      "source": [
        "!curl -X PUT --data-binary @RandomForestIris.pmml -H \"Content-type: text/xml\" http://localhost:8081/openscoring/model/RandomForestIris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt9VyKdGFJpB"
      },
      "source": [
        "## Se define la UDF para la inferencia en tiempo real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d95tSDA2gJt2"
      },
      "outputs": [],
      "source": [
        "def make_model_prediction(sepal_width, sepal_length, petal_width, petal_length):\n",
        "  body = {\n",
        "    'id': f'record-{uuid4()}',\n",
        "    'arguments': {'sepal_width': sepal_width, 'sepal_length': sepal_length,\n",
        "             'petal_width': petal_width, 'petal_length': petal_length}\n",
        "          }\n",
        "\n",
        "  headers = {\"Content-type\": \"application/json\"}\n",
        "  response = requests.post(url='http://localhost:8081/openscoring/model/RandomForestIris', json=body, headers=headers)\n",
        "\n",
        "  return response.json()['results']['prediction']\n",
        "\n",
        "\n",
        "make_model_prediction_udf = udf(make_model_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EROGXxkjFMZh"
      },
      "source": [
        "## Se carga el modelo de `IndexToString` para pasar de la predicción numérica a la clase real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_gc_QYWb0_E"
      },
      "outputs": [],
      "source": [
        "index_to_class = IndexToString.load('./index_to_string')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NuWhqfVWbsy"
      },
      "outputs": [],
      "source": [
        "!mkdir results\n",
        "!mkdir results2\n",
        "!mkdir filestream_results\n",
        "!mkdir checkpoint\n",
        "!mkdir checkpoint2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBe6ADn6FTcB"
      },
      "source": [
        "## Leyendo de Kafka\n",
        "\n",
        "En este paso se ejecuta la lectura de Kafka, pero no automáticamente ni directamente, sino que se comienza un proceso que va a leer la data cuando llegue, va a desarmar el json, va a ejecutar la predicción, y va a hacer un join con la data original para ver si se predijo bien o no. Esto se ejecuta de manera asincrónica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1sf4mn-VzZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15741539-c426-4fac-f6d3-ba3fa8cedf6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.StreamingQuery at 0x7ca161b380d0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "source_schema = StructType().add('sepal_length', DoubleType()) \\\n",
        "  .add('sepal_width', DoubleType()) \\\n",
        "  .add('petal_length', DoubleType()) \\\n",
        "  .add('petal_width', DoubleType())\n",
        "\n",
        "streaming_df = spark \\\n",
        "  .readStream \\\n",
        "  .format(\"kafka\") \\\n",
        "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
        "  .option(\"subscribe\", \"iris\") \\\n",
        "  .load() \\\n",
        "  .select(from_json(col('value').cast('string'), source_schema).alias('value')) \\\n",
        "  .select(col('value.sepal_length').alias('sepal_length'),\n",
        "          col('value.sepal_width').alias('sepal_width'),\n",
        "          col('value.petal_length').alias('petal_length'),\n",
        "          col('value.petal_width').alias('petal_width')) \\\n",
        "  .select('*', make_model_prediction_udf('sepal_width', 'sepal_length',\n",
        "                                    'petal_width', 'petal_length') \\\n",
        "          .cast(IntegerType()).alias('prediction')) \\\n",
        "  .join(iris_df, ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']) \\\n",
        "  .withColumnRenamed('variety', 'original_class')\n",
        "\n",
        "streaming_df = index_to_class.transform(streaming_df)\n",
        "\n",
        "streaming_df.writeStream \\\n",
        "  .outputMode('append') \\\n",
        "  .format('json') \\\n",
        "  .option('path', 'results') \\\n",
        "  .option('header', 'true') \\\n",
        "  .option(\"checkpointLocation\", \"checkpoint\") \\\n",
        "  .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-590oVVpiZ_"
      },
      "outputs": [],
      "source": [
        "!sleep 10\n",
        "\n",
        "def write_result(rdd):\n",
        "  if not rdd.isEmpty():\n",
        "    spark.read.json(rdd) \\\n",
        "      .write \\\n",
        "      .format('json') \\\n",
        "      .option('path',  f'filestream_results/{time.time()}') \\\n",
        "      .option('header', 'true') \\\n",
        "      .save()\n",
        "\n",
        "ssc = StreamingContext(sc, 1)\n",
        "\n",
        "lines = ssc.textFileStream('file:///content/results2')\n",
        "\n",
        "lines.map(lambda x: loads(x)) \\\n",
        "  .map(lambda x: (x['sepal_width'], 1)) \\\n",
        "  .reduceByKey(lambda a, b: a + b) \\\n",
        "  .map(lambda x: dumps({'sepal_width': x[0], 'count': x[1]})) \\\n",
        "  .foreachRDD(lambda rdd: write_result(rdd))\n",
        "\n",
        "lines.pprint()\n",
        "\n",
        "ssc.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1Jp_VC1FwWQ"
      },
      "source": [
        "## Escribiendo a Kafka\n",
        "\n",
        "Ya iniciado el paso anterior, se escribe a Kafka el dataset completo. Esto llegara al proceso anterior y se hara la predicción. Para ver los resutlados, ir a la carpeta ubicada en `/content/results` y buscar los archivos que comienzan en `part-000...`. Ahi estan los resultados en formato json.\n",
        "\n",
        "En este caso como clave se eligió el hash de los valores de entrada. La realidad es que no hace diferencia ya que solo hay una partición."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bi5cA5TiIuL"
      },
      "outputs": [],
      "source": [
        "!sleep 10\n",
        "\n",
        "iris_df.select(to_json(struct('sepal_width', 'sepal_length', 'petal_width', 'petal_length')).alias('value')) \\\n",
        "  .withColumn('key', md5('value')) \\\n",
        "  .selectExpr(\"key\", \"CAST(value AS STRING)\") \\\n",
        "  .write \\\n",
        "  .format(\"kafka\") \\\n",
        "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
        "  .option(\"topic\", \"iris\") \\\n",
        "  .save()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sleep 10\n",
        "!cp results/*.json ./results2\n",
        "ssc.awaitTermination()"
      ],
      "metadata": {
        "id": "ScbiCPwjMPF5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}